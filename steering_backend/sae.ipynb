{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sae_lens import HookedSAETransformer, SAE\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm # Use tqdm.auto for notebook compatibility\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 40.40it/s]\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\", device=device)\n",
    "\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae loaded\n",
      "{'architecture': 'jumprelu', 'd_in': 2304, 'd_sae': 16384, 'activation_fn_str': 'relu', 'apply_b_dec_to_input': False, 'finetuning_scaling_factor': False, 'context_size': 1024, 'model_name': 'gemma-2-2b', 'hook_name': 'blocks.20.hook_resid_post', 'hook_layer': 20, 'hook_head_index': None, 'prepend_bos': True, 'dataset_path': 'monology/pile-uncopyrighted', 'dataset_trust_remote_code': True, 'normalize_activations': None, 'dtype': 'float32', 'device': 'mps', 'sae_lens_training_version': None, 'activation_fn_kwargs': {}, 'neuronpedia_id': None, 'model_from_pretrained_kwargs': {}, 'seqpos_slice': (None,)}\n"
     ]
    }
   ],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\",\n",
    "    sae_id = \"layer_20/width_16k/average_l0_71\",\n",
    ")\n",
    "\n",
    "sae = sae.to(device)\n",
    "\n",
    "print(\"sae loaded\")\n",
    "\n",
    "print(sae.cfg.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      modelId                  layer  feature  \\\n",
      "0  gemma-2-2b  20-gemmascope-res-16k    14403   \n",
      "1  gemma-2-2b  20-gemmascope-res-16k    14403   \n",
      "2  gemma-2-2b  20-gemmascope-res-16k    14403   \n",
      "3  gemma-2-2b  20-gemmascope-res-16k    10131   \n",
      "4  gemma-2-2b  20-gemmascope-res-16k    10133   \n",
      "\n",
      "                                         description  \\\n",
      "0  phrases or sentences that introduce lists, exa...   \n",
      "1  references to numerical sports scores and resu...   \n",
      "2  text related to sports accomplishments and sta...   \n",
      "3  phrases referring to being fluent in a languag...   \n",
      "4  words related to scientific studies and proces...   \n",
      "\n",
      "         explanationModelName            typeName  \n",
      "0  claude-3-5-sonnet-20240620  oai_token-act-pair  \n",
      "1              gemini-1.5-pro  oai_token-act-pair  \n",
      "2                 gpt-4o-mini  oai_token-act-pair  \n",
      "3            gemini-1.5-flash  oai_token-act-pair  \n",
      "4            gemini-1.5-flash  oai_token-act-pair  \n"
     ]
    }
   ],
   "source": [
    "# get explanations\n",
    "url = \"https://www.neuronpedia.org/api/explanation/export?modelId=gemma-2-2b&saeId=20-gemmascope-res-16k\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# convert to pandas\n",
    "data = response.json()\n",
    "explanations_df = pd.DataFrame(data)\n",
    "# rename index to \"feature\"\n",
    "explanations_df.rename(columns={\"index\": \"feature\"}, inplace=True)\n",
    "explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "explanations_df[\"description\"] = explanations_df[\"description\"].apply(\n",
    "    lambda x: x.lower()\n",
    ")\n",
    "\n",
    "print(explanations_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blocks.20.hook_resid_post.hook_sae_input', torch.Size([1, 23, 2304])), ('blocks.20.hook_resid_post.hook_sae_acts_pre', torch.Size([1, 23, 16384])), ('blocks.20.hook_resid_post.hook_sae_acts_post', torch.Size([1, 23, 16384])), ('blocks.20.hook_resid_post.hook_sae_recons', torch.Size([1, 23, 2304])), ('blocks.20.hook_resid_post.hook_sae_output', torch.Size([1, 23, 2304]))]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello, how are you? I am a human but in a world of AI, I am a robot.\"\n",
    "\n",
    "_, cache = model.run_with_cache_with_saes(prompt, saes=[sae])\n",
    "\n",
    "print([(k, v.shape) for k, v in cache.items() if \"sae\" in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features firing at the last token position for blocks.20.hook_resid_post.hook_sae_acts_post:\n",
      "\n",
      "Feature 1858: Activation = 71.8360\n",
      "  Explanation: punctuation marks and sentence endings\n",
      "\n",
      "Feature 6631: Activation = 57.2718\n",
      "  Explanation: the beginning of a text or important markers in a document\n",
      "\n",
      "Feature 8450: Activation = 56.2794\n",
      "  Explanation: keywords related to the development and implications of artificial intelligence and autonomous technologies\n",
      "\n",
      "Feature 2229: Activation = 53.6663\n",
      "  Explanation:  punctuation marks, particularly periods and dollar signs\n",
      "\n",
      "Feature 11133: Activation = 51.0430\n",
      "  Explanation: references to personal experiences and advice\n"
     ]
    }
   ],
   "source": [
    "# Get activations for the relevant hook point (layer 20 SAE post-activation)\n",
    "layer_hook = 'blocks.20.hook_resid_post.hook_sae_acts_post'\n",
    "activations = cache[layer_hook][0, -1, :] # Batch 0, last token\n",
    "\n",
    "# Get the top 5 features\n",
    "k = 5\n",
    "top_vals, top_inds = torch.topk(activations, k)\n",
    "\n",
    "print(f\"Top {k} features firing at the last token position for {layer_hook}:\")\n",
    "for val, ind in zip(top_vals, top_inds):\n",
    "    feature_id = ind.item()\n",
    "    activation_value = val.item()\n",
    "\n",
    "    # Find explanations for this feature ID in the dataframe\n",
    "    # Note: Explanations might be duplicated if multiple sources provided them\n",
    "    feature_explanations = explanations_df[explanations_df['feature'] == feature_id]\n",
    "\n",
    "    print(f'\\nFeature {feature_id}: Activation = {activation_value:.4f}')\n",
    "    if not feature_explanations.empty:\n",
    "        # Print the first explanation found (or iterate through all if needed)\n",
    "        # Using .unique() in case there are multiple rows for the same feature\n",
    "        unique_descriptions = feature_explanations['description'].unique()\n",
    "        for desc in unique_descriptions:\n",
    "             print(f\"  Explanation: {desc}\")\n",
    "    else:\n",
    "        print(f\"  Explanation: Not found in the loaded explanations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Steering Functions ---\n",
    "\n",
    "# Although we fetch max_act from Neuronpedia, keep the function for reference\n",
    "def find_max_activation(model, sae, activation_store, feature_idx, num_batches=100):\n",
    "    \"\"\"\n",
    "    Find the maximum activation for a given feature index. This is useful for\n",
    "    calibrating the right amount of the feature to add.\n",
    "    Requires an activation_store object (not provided in this script).\n",
    "    \"\"\"\n",
    "    max_activation = 0.0\n",
    "    if activation_store is None:\n",
    "        print(\"Warning: activation_store not provided to find_max_activation.\")\n",
    "        return max_activation # Return 0 if no store\n",
    "\n",
    "    pbar = tqdm(range(num_batches))\n",
    "    for _ in pbar:\n",
    "        tokens = activation_store.get_batch_tokens()\n",
    "\n",
    "        _, cache = model.run_with_cache(\n",
    "            tokens,\n",
    "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
    "            names_filter=[sae.cfg.hook_name],\n",
    "        )\n",
    "        sae_in = cache[sae.cfg.hook_name]\n",
    "        # Note: encode uses W_enc, not W_dec like the steering vector\n",
    "        feature_acts = sae.encode(sae_in) # Shape: [batch, seq, d_sae]\n",
    "\n",
    "        # Flatten batch and sequence dimensions\n",
    "        feature_acts = feature_acts.flatten(0, 1) # Shape: [batch*seq, d_sae]\n",
    "        if feature_acts.shape[0] > 0: # Ensure there are activations\n",
    "            batch_max_activation = feature_acts[:, feature_idx].max().item()\n",
    "            max_activation = max(max_activation, batch_max_activation)\n",
    "\n",
    "        pbar.set_description(f\"Max activation: {max_activation:.4f}\")\n",
    "\n",
    "    return max_activation\n",
    "\n",
    "\n",
    "def steering(\n",
    "    activations, hook, steering_strength=1.0, steering_vector=None, max_act=1.0\n",
    "    ):\n",
    "    \"\"\"Applies steering vector to the activations at a hook point.\"\"\"\n",
    "    if steering_vector is None:\n",
    "        return activations\n",
    "\n",
    "    # Ensure steering_vector is on the same device and dtype as activations\n",
    "    steering_vector = steering_vector.to(activations.device, dtype=activations.dtype)\n",
    "\n",
    "    # Add the scaled steering vector\n",
    "    # Note: We add the steering vector directly. If the feature naturally activates,\n",
    "    # this adds to the existing activation.\n",
    "    activations = activations + max_act * steering_strength * steering_vector\n",
    "    return activations\n",
    "\n",
    "\n",
    "def generate_with_steering(\n",
    "    model,\n",
    "    sae,\n",
    "    prompt,\n",
    "    steering_feature,\n",
    "    max_act,\n",
    "    steering_strength=1.0,\n",
    "    max_new_tokens=95,\n",
    "    ):\n",
    "    \"\"\"Generates text with steering applied at the SAE hook point.\"\"\"\n",
    "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
    "\n",
    "    # Get the steering vector (decoder weight for the feature)\n",
    "    steering_vector = sae.W_dec[steering_feature]\n",
    "\n",
    "    # Create the partial hook function with steering parameters\n",
    "    steering_hook = partial(\n",
    "        steering,\n",
    "        steering_vector=steering_vector,\n",
    "        steering_strength=steering_strength,\n",
    "        max_act=max_act,\n",
    "    )\n",
    "\n",
    "    # Generate text within the hook context\n",
    "    # Use model.cfg.device to check device, ensuring it's a string for comparison\n",
    "    current_device_str = str(model.cfg.device)\n",
    "    stop_eos = False if current_device_str == \"mps\" else True\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, steering_hook)]):\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop_at_eos=stop_eos,\n",
    "            prepend_bos=sae.cfg.prepend_bos,\n",
    "        )\n",
    "\n",
    "    # Decode the entire generated sequence\n",
    "    return model.tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching feature data from: https://www.neuronpedia.org/api/feature/gemma-2-2b/20-gemmascope-res-16k/8450\n",
      "Warning: 'max_activation' key not found in Neuronpedia API response for feature 8450. Available keys: ['modelId', 'layer', 'index', 'sourceSetName', 'creatorId', 'createdAt', 'maxActApprox', 'hasVector', 'vector', 'vectorLabel', 'vectorDefaultSteerStrength', 'hookName', 'topkCosSimIndices', 'topkCosSimValues', 'neuron_alignment_indices', 'neuron_alignment_values', 'neuron_alignment_l1', 'correlated_neurons_indices', 'correlated_neurons_pearson', 'correlated_neurons_l1', 'correlated_features_indices', 'correlated_features_pearson', 'correlated_features_l1', 'neg_str', 'neg_values', 'pos_str', 'pos_values', 'frac_nonzero', 'freq_hist_data_bar_heights', 'freq_hist_data_bar_values', 'logits_hist_data_bar_heights', 'logits_hist_data_bar_values', 'decoder_weights_dist', 'umap_cluster', 'umap_log_feature_sparsity', 'umap_x', 'umap_y', 'model', 'lists', 'creator', 'source', 'sourceSet', 'comments', 'activations', 'explanations']. Using default value: 1.0\n",
      "\n",
      "Generating text without steering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/95 [02:06<1:37:41, 63.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m current_device_str = \u001b[38;5;28mstr\u001b[39m(model.cfg.device)\n\u001b[32m     41\u001b[39m stop_eos_normal = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m current_device_str == \u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m normal_text_output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop_at_eos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop_eos_normal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[43m=\u001b[49m\u001b[43msae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Added temperature for consistency\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Added top_p for consistency\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Decode the full output sequence\u001b[39;00m\n\u001b[32m     52\u001b[39m normal_text = model.tokenizer.decode(normal_text_output[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/steering/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/steering/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:2235\u001b[39m, in \u001b[36mHookedTransformer.generate\u001b[39m\u001b[34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, use_past_kv_cache, prepend_bos, padding_side, return_type, verbose)\u001b[39m\n\u001b[32m   2230\u001b[39m pos_offset = \u001b[38;5;28mself\u001b[39m.get_pos_offset(past_kv_cache, batch_size)\n\u001b[32m   2232\u001b[39m tokens = torch.zeros((embeds.size(\u001b[32m0\u001b[39m), embeds.size(\u001b[32m1\u001b[39m))).to(torch.int)\n\u001b[32m   2233\u001b[39m attention_mask = \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_attention_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2234\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprepend_bos\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m2235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2236\u001b[39m residual, shortformer_pos_embed = \u001b[38;5;28mself\u001b[39m.get_residual(\n\u001b[32m   2237\u001b[39m     embeds,\n\u001b[32m   2238\u001b[39m     pos_offset,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2241\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   2242\u001b[39m )\n\u001b[32m   2244\u001b[39m \u001b[38;5;66;03m# While generating, we keep generating logits, throw away all but the final logits,\u001b[39;00m\n\u001b[32m   2245\u001b[39m \u001b[38;5;66;03m# and then use those logits to sample from the distribution We keep adding the\u001b[39;00m\n\u001b[32m   2246\u001b[39m \u001b[38;5;66;03m# sampled tokens to the end of tokens.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Steering Example ---\n",
    "\n",
    "# Choose a feature to steer (e.g., the AI feature 8450 found earlier)\n",
    "steering_feature = 8450\n",
    "# You can also try feature 1858 (punctuation/endings) or 6631 (text beginnings)\n",
    "\n",
    "# --- Get Max Activation from Neuronpedia API ---\n",
    "neuronpedia_model_id = sae.cfg.model_name # Should be 'gemma-2-2b'\n",
    "# Construct the layer/SAE ID string used by Neuronpedia API\n",
    "# This format might need adjustment based on the exact SAE release/naming in Neuronpedia\n",
    "neuronpedia_layer_id = \"20-gemmascope-res-16k\"\n",
    "feature_url = f\"https://www.neuronpedia.org/api/feature/{neuronpedia_model_id}/{neuronpedia_layer_id}/{steering_feature}\"\n",
    "\n",
    "print(f\"Fetching feature data from: {feature_url}\")\n",
    "max_act = 1.0 # Default value if API fails or key missing\n",
    "try:\n",
    "    response = requests.get(feature_url)\n",
    "    response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "    feature_data = response.json()\n",
    "    # Attempt to extract max activation - common key name is 'max_activation'\n",
    "    if 'max_activation' in feature_data:\n",
    "        max_act = float(feature_data['max_activation'])\n",
    "        print(f\"Successfully fetched max activation: {max_act:.4f}\")\n",
    "    else:\n",
    "        print(f\"Warning: 'max_activation' key not found in Neuronpedia API response for feature {steering_feature}. Available keys: {list(feature_data.keys())}. Using default value: {max_act}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching data from Neuronpedia: {e}. Using default max_act={max_act}\")\n",
    "except ValueError:\n",
    "    print(f\"Error converting max_activation ('{feature_data.get('max_activation')}') to float. Using default max_act={max_act}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}. Using default max_act={max_act}\")\n",
    "\n",
    "# --- Generate Text ---\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Generate text without steering for comparison\n",
    "print(\"\\nGenerating text without steering...\")\n",
    "# Use model.cfg.device to check device, ensuring it's a string for comparison\n",
    "current_device_str = str(model.cfg.device)\n",
    "stop_eos_normal = False if current_device_str == \"mps\" else True\n",
    "\n",
    "normal_text_output = model.generate(\n",
    "    prompt,\n",
    "    max_new_tokens=95,\n",
    "    stop_at_eos=stop_eos_normal,\n",
    "    prepend_bos=sae.cfg.prepend_bos,\n",
    "    temperature=0.7, # Added temperature for consistency\n",
    "    top_p=0.9, # Added top_p for consistency\n",
    ")\n",
    "# Decode the full output sequence\n",
    "normal_text = model.tokenizer.decode(normal_text_output[0], skip_special_tokens=True)\n",
    "print(\"Normal text (without steering):\")\n",
    "print(normal_text)\n",
    "print(\"-\" * 30) # Separator\n",
    "\n",
    "# Generate text with steering\n",
    "print(\"\\nGenerating text with steering...\")\n",
    "steered_text = generate_with_steering(\n",
    "    model,\n",
    "    sae,\n",
    "    prompt,\n",
    "    steering_feature,\n",
    "    max_act,\n",
    "    steering_strength=2.0, # Example strength - adjust as needed\n",
    "    max_new_tokens=95\n",
    ")\n",
    "print(f\"Steered text (feature {steering_feature}, strength 2.0, max_act {max_act:.4f}):\")\n",
    "print(steered_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
